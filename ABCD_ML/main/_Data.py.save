"""
_Data.py
====================================
Main class extension file for the Loading functionality methods of
the ABCD_ML class.
"""
import pandas as pd
import numpy as np

from ..helpers.Data_Helpers import (process_binary_input,
                                    process_ordinal_input,
                                    process_categorical_input,
                                    process_float_input,
                                    process_multilabel_input,
                                    get_unused_drop_val,
                                    filter_float_by_outlier,
                                    filter_float_by_std,
                                    filter_float_df_by_outlier,
                                    filter_float_df_by_std,
                                    drop_duplicate_cols,
                                    get_top_substrs,
                                    proc_datatypes,
                                    proc_args,
                                    get_common_name)


def Set_Default_Load_Params(self, dataset_type='default', subject_id='default',
                            eventname='default', eventname_col='default',
                            overlap_subjects='default', na_values='default',
                            drop_na='default', drop_or_na='default'):
    ''' This function is used to define default values for a series of
    params accessible to all or most of the different loading functions.
    By setting common values here, it reduces the need to repeat params within
    each loader (e.g. Load_Data, Load_Targets, ect...)

    Parameters
    ----------
    dataset_type : {'basic', 'explorer', 'custom'}, optional
        The dataset_type / file-type to load from.
        Dataset types are,

        - 'basic'
            ABCD2p0NDA style (.txt and tab seperated).
            Typically the default columns, and therefore not neuroimaging
            data, will be dropped, also not including the eventname column.

        - 'explorer'
            2.0_ABCD_Data_Explorer style (.csv and comma seperated).
            The first 2 columns before self.subject_id
            (typically the default columns, and therefore not neuroimaging
            data - also not including the eventname column), will be dropped.

        - 'custom'
            A user-defined custom dataset. Right now this is only.
            supported as a comma seperated file, with the subject names in a
            column called self.subject_id, and can optionally have
            'eventname'. No columns will be dropped,
            (except eventname) or unless specific drop keys are passed.

        If loading multiple locs as a list, dataset_type can be a list with
        inds corresponding to which datatype for each loc.

        if 'default', and not already defined, set to 'basic'
        (default = 'default')

    subject_id : str, optional
        The name of the column with unique subject ids in different
        dataset, for default ABCD datasets this is 'src_subject_id',
        but if a user wanted to load and work with a different dataset,
        they just need to change this accordingly
        (in addition to setting eventname most likely to None and
        use_default_subject_ids to False)

        if 'default', and not already defined, set to 'src_subject_id'.
        (default = 'default')

    eventname : value, list of values or None, optional
        Optional value to provide, specifying to optional keep certain rows
        when reading data based on the eventname flag, where eventname
        is the value and eventname_col is the name of the value.

        If a list of values are passed, then it will be treated as keeping a
        row if that row's value within the eventname_col is equal to ANY
        of the passed eventname values.

        As ABCD is a longitudinal study, this flag lets you select only
        one specific time point, or if set to None, will load everything.

        For selecting only baseline imagine data one might consider
        setting this param to 'baseline_year_1_arm_1'.

        if 'default', and not already defined, set to None.
        (default = 'default')

    eventname_col : str or None, optional
        If an eventname is provided, this param refers to
        the column name containing the eventname. This could
        also be used along with eventname to be set to any
        arbitrary value, in order to perform selection by specific
        column value.

        Note: The eventname col is dropped after proc'ed!

        if 'default', and not already defined, set to 'eventname'
        (default = 'default')

    overlap_subjects : bool, optional
        This parameter dictates when loading data, covars, targets or strat
        (after initial basic proc and/or merge w/ other passed loc's),
        if the loaded data should be restricted to only the
        overlapping subjects from previously loaded data, targets, covars
        or strat - important when performing intermediate proc.
        If False, then all subjects will
        be kept throughout the rest of the optional processing - and only
        merged at the end AFTER processing has been done.

        Note: Inclusions and Exclusions are always applied regardless of this
        parameter.

        if 'default', and not already defined, set to False
        (default = 'default')

    na_values : list, optional
        Additional values to treat as NaN, by default ABCD specific
        values of '777' and '999' are treated as NaN,
        and those set to default by pandas 'read_csv' function.
        Note: if new values are passed here,
        it will override these default '777' and '999' NaN values,
        so if it desired to keep these, they should be passed explicitly,
        along with any new values.

        if 'default', and not already defined, set to ['777', '999']
        (default = 'default')

    drop_na : bool, int, float or 'default', optional
        This setting sets the value for drop_na,
        which is used when loading data and covars only!

        If set to True, then will drop any row within the loaded
        data if there are any NaN! If False, the will not drop any
        rows for missing values.

        If an int or float, then this means some NaN entries
        will potentially be preserved! Missing data imputation
        will therefore be required later on!

        If an int > 1, then will drop any row with more than drop_na
        NaN values. If a float, will determine the drop threshold as
        a percentage of the possible values, where 1 would not drop any
        rows as it would require the number of columns + 1 NaN, and .5
        would require that more than half the column entries are NaN in
        order to drop that row.

        if 'default', and not already defined, set to True
        (default = 'default')

    drop_or_na : {'drop', 'na'}, optional

        This setting sets the value for drop_na,
        which is used when loading data and covars only!

        filter_outlier_percent, or when loading a binary variable
        in load covars and more then two classes are present - are both
        instances where rows/subjects are by default dropped.
        If drop_or_na is set to 'na', then these values will instead be
        set to 'na' rather then the whole row dropped!

        Otherwise, if left as default value of 'drop', then rows will be
        dropped!

        if 'default', and not already defined, set to 'drop'
        (default = 'default')
    '''

    if dataset_type != 'default':
        self.default_load_params['dataset_type'] = dataset_type
    elif 'dataset_type' not in self.default_load_params:
        self.default_load_params['dataset_type'] = 'basic'
        self._print('No default dataset_type passed, set to "basic"')

    if subject_id != 'default':
        self.default_load_params['subject_id'] = subject_id
    elif 'subject_id' not in self.default_load_params:
        self.default_load_params['subject_id'] = 'src_subject_id'
        self._print('No default subject_id passed, set to "src_subject_id"')

    if eventname != 'default':
        self.default_load_params['eventname'] = eventname
    elif 'eventname' not in self.default_load_params:
        self.default_load_params['eventname'] = None
        self._print('No default eventname passed,',
                    'set to None')

    if eventname_col != 'default':
        self.default_load_params['eventname_col'] = eventname_col
    elif 'eventname_col' not in self.default_load_params:
        self.default_load_params['eventname_col'] = 'eventname'
        self._print('No default eventname_col passed,',
                    'set to "eventname"')

    if overlap_subjects != 'default':
        self.default_load_params['overlap_subjects'] = overlap_subjects
    elif 'overlap_subjects' not in self.default_load_params:
        self.default_load_params['overlap_subjects'] = False
        self._print('No default overlap_subjects passed, set to False')

    if na_values != 'default':
        self.default_load_params['na_values'] = na_values
    elif 'na_values' not in self.default_load_params:
        self.default_load_params['na_values'] = ['777', '999']
        self._print('No default na_values passed, set to ["777", "999"]')

    if drop_na != 'default':
        self.default_load_params['drop_na'] = drop_na
    elif 'drop_na' not in self.default_load_params:
        self.default_load_params['drop_na'] = True
        self._print('No default drop_na passed, set to True')

    if drop_or_na != 'default':
        self.default_load_params['drop_or_na'] = drop_or_na
    elif 'drop_or_na' not in self.default_load_params:
        self.default_load_params['drop_or_na'] = 'drop'
        self._print('No default drop_or_na passed, set to "drop"')

    self._print('Default load params set within self.default_load_params.')
    self._print()

    # subject_id='src_subject_id',
    # eventname='baseline_year_1_arm_1',
    # default_dataset_type='basic',
    # drop_nan=True,
    # default_na_values=['777', '999'],


def _make_load_params(self, args):

    if len(self.default_load_params) == 0:

        # Set default load params with default vals
        self._print('Setting default load params, as they have not been set!')
        self._print()
        self.Set_Default_Load_Params()
        self._print('To change the default load params, call',
                    'self.Set_Default_Load_Params()')
        self._print()

    load_params = self.default_load_params.copy()

    for key in args:
        if key in load_params:
            if args[key] != 'default' and args[key] != 'self':
                load_params[key] = args[key]

    return load_params


def Load_Name_Map(self, name_map=None, loc=None, dataset_type='default',
                  source_name_col="NDAR name",
                  target_name_col="REDCap name/NDA alias",
                  na_values='default',
                  clear_existing=False):
    '''Loads a mapping dictionary for loading column names. Either a loc
    or name_map must be passed! Note: If both a name_map and loc are passed,
    the name_map will be loaded first, then updated with values from the loc.

    Parameters
    ----------
    name_map : dict or None, optional
        A dictionary containing the mapping to be passed directly.
        Set to None if using loc instead!

        (default = None)

    loc : str, Path or None, optional
        The location of the csv file which contains the mapping.

        (default = None)

    dataset_type :

    source_name_col : str, optional
        The column name with the file which lists names to be changed.

        (default = "NDAR name")

    target_name_col : str, optional
        The column name within the file which lists the new name.

        (default = "REDCap name/NDA alias")

    na_values :

    clear_existing : bool, optional
        If set to True, will clear the existing loaded name_map, otherwise the
        name_map dictionary will be updated if already loaded!
    '''

    if clear_existing:
        self.Clear_Name_Map()

    if name_map is not None:

        if len(self.name_map) > 0:
            self._print('Updating existing name_map with new!')
        else:
            self._print('Loading new name_map')

        self.name_map.update(name_map)

    if loc is not None:

        load_params = self._make_load_params(args=locals())

        # Load mapping based on dataset type
        mapping = self._load(loc, load_params['dataset_type'],
                             load_params['na_values'])

        try:
            name_map_from_loc = dict(zip(mapping[source_name_col],
                                         mapping[target_name_col]))

            if len(self.name_map) > 0:
                self._print('Updating existing name_map with new from file!')
            else:
                self._print('Loading new name_map from file!')

            self.name_map.update(name_map_from_loc)

        except KeyError:
            print('Error: One or both provided column names do not exist!')
            print('Name map not loaded from loc!')


def Load_Data(self, loc=None, df=None, dataset_type='default', drop_keys=None,
              inclusion_keys=None, subject_id='default', eventname='default',
              eventname_col='default', overlap_subjects='default',
              na_values='default', drop_na='default', drop_or_na='default',
              filter_outlier_percent=None, filter_outlier_std=None,
              unique_val_drop=None, unique_val_warn=.05,
              drop_col_duplicates=None,
              clear_existing=False):
    """Class method for loading ROI-style data, assuming all loaded
    columns are continuous / float datatype.

    Parameters
    ----------
    loc : str Path, list of or None, optional
        The location of the file to load data load from.
        If passed a list, then will load each loc in the list,
        and will assume them all to be of the same dataset_type if one
        dataset_type is passed, or if they differ in type, a list must be
        passed to dataset_type with the different types in order.

        Note: some proc will be done on each loaded dataset before merging
        with the rest (duplicate subjects, proc for eventname ect...), but
        other dataset loading behavior won't occur until after the merge,
        e.g., dropping cols by key, filtering for outlier, ect...

    df : pandas DataFrame or None, optional
        This parameter represents the option for the user to pass in
        a raw custom dataframe. A loc and/or a df must be passed.

        When pasing a raw DataFrame, the loc and dataset_type
        param will be ignored, as those are for loading data from a file.
        Otherwise, it will be treated the same as
        if loading from a file, which means, there should be a column within
        the passed dataframe with subject_id, and e.g. if eventname params are
        passed, they will be applied along with any other proc. specified.

    dataset_type :
    drop_keys : str, list or None, optional
        A list of keys to drop columns by, where if any key given in a columns
        name, then that column will be dropped. If a str, then same behavior,
        just with one col.
        (Note: if a name mapping exists, this drop step will be
        conducted after renaming)

        (default = None)

    inclusion_keys : str, list or None, optional
        A list of keys in which to only keep a loaded data
        column if ANY of the passed inclusion_keys are present
        within that column name.

        If passed only with drop_keys will be proccessed second.

        (Note: if a name mapping exists, this drop step will be
        conducted after renaming)

        (default = None)

    subject_id :
    eventname :
    eventname_col :
    overlap_subjects :
    na_values :
    drop_na :
    drop_or_na :
    filter_outlier_percent : int, float, tuple or None, optional
        *For float data only.*
        A percent of values to exclude from either end of the
        targets distribution, provided as either 1 number,
        or a tuple (% from lower, % from higher).
        set `filter_outlier_percent` to None for no filtering.
        If over 1 then treated as a percent, if under 1, then
        used directly.

        If drop_or_na == 'drop', then all rows/subjects with >= 1
        value(s) found outside of the percent will be dropped.
        Otherwise, if drop_or_na = 'na', then any outside values
        will be set to NaN.

        (default = None)

    filter_outlier_std : int, float, tuple or None, optional
        *For float data only.*
        Determines outliers as data points within each column where their
        value is less than the mean of the column - `filter_outlier_std[0]`
        * the standard deviation of the column,
        and greater than the mean of the column + `filter_outlier_std[1]`
        * the standard deviation of the column.

        If a singler number is passed, that number is applied to both the lower
        and upper range. If a tuple with None on one side is passed, e.g.
        (None, 3), then nothing will be taken off that lower or upper bound.

        If drop_or_na == 'drop', then all rows/subjects with >= 1
        value(s) found will be dropped. Otherwise, if drop_or_na = 'na',
        then any outside values will be set to NaN.

        (default = None)

    unique_val_drop : int, float None, optional
        This parameter allows you to drops columns within loaded data
        where there are under a certain threshold of unique values.

        The threshold is determined by the passed value as first converted
        to a float between 0 and 1 (e.g. if passed 5, to .05), and then
        computed as unique_val_drop * len(data). Any column with less unique
        values then this threshold will be dropped

        (default = None)

    unique_val_warn : int or float, optional
        This parameter is simmilar to unique_val_drop, but only
        warns about columns with under the threshold (see unique_val_drop for
        how the threshold is computed) unique vals.

        (default = .05)

    drop_col_duplicates : float or None/False, optional
        If set to None, will not drop any.
        If float, then pass a value between 0 and 1,
        where if two columns within data
        are correlated >= to `corr_thresh`, the second column is removed.

        A value of 1 will instead make a quicker direct =='s comparison.

        Note: This param just drops duplicated within the just loaded data.
        You can call self.Drop_Data_Duplicates() to drop duplicates across
        all loaded data.

        Be advised, this functionality runs rather slow when there are ~500+
        columns to compare!

        (default = None)

    clear_existing : bool, optional
        If this parameter is set to True, then any existing
        loaded data will first be cleared before loading new data!

        .. WARNING::
            If any subjects have been dropped from a different place,
            e.g. targets, then simply reloading / clearing existing data
            might result in computing a misleading overlap of final
            valid subjects. Reloading should therefore be best used
            right after loading the original data, or if not possible,
            then reloading the notebook or re-running the script.

        (default = False)
    """

    # Clear existing if requested, otherwise append to
    if clear_existing:
        self.Clear_Data()

    # Get the common load params as a mix of user-passed + default values
    load_params = self._make_load_params(args=locals())

    # Load in the raw dataframe - based on dataset type and/or passed user df
    data = self._load_datasets(loc, df, load_params)

    # Set to only overlap subjects if passed
    data = self._set_overlap(data, load_params['overlap_subjects'])

    # Drop cols by drop keys and inclusion_keys
    data = self._drop_data_cols(data, drop_keys, inclusion_keys)

    # Handle missing data
    data = self._drop_na(data, load_params['drop_na'])

    # Filter based on passed filter_outlier_percent
    data = self._filter_data_cols(data, filter_outlier_percent,
                                  filter_outlier_std,
                                  load_params['drop_or_na'])

    # Drop/warn about number of unique cols
    data = self._proc_data_unique_cols(data, unique_val_drop, unique_val_warn)

    # Drop column duplicates if param passed
    data = drop_duplicate_cols(data, drop_col_duplicates)

    # Show final na info after all proc
    self._show_na_info(data)

    # Display final shape
    self._print('loaded shape: ', data.shape)

    # Merge self.data with new loaded data
    self.data = self._merge_existing(self.data, data)

    # Process new loaded subjects
    self._process_new(self.low_memory_mode)


def Load_Targets(self, loc=None, df=None, col_name=None, data_type=None,
                 dataset_type='default', subject_id='default',
                 eventname='default', eventname_col='default',
                 overlap_subjects='default', filter_outlier_percent=None,
                 filter_outlier_std=None, categorical_drop_percent=None,
                 na_values='default', clear_existing=False):
    '''Loads in targets, the outcome / variable(s) to predict.

    Parameters
    ----------
    loc : str, Path or None, optional
        The location of the file to load targets load from.

        Either loc or df must be set, but they both cannot be set!

        (default = None)

    df : pandas DataFrame or None, optional
        This parameter represents the option for the user to pass in
        a raw custom dataframe. A loc and/or a df must be passed.

        When pasing a raw DataFrame, the loc and dataset_type
        param will be ignored, as those are for loading from a file.
        Otherwise, it will be treated the same as
        if loading from a file, which means, there should be a column within
        the passed dataframe with subject_id, and e.g. if eventname params are
        passed, they will be applied along with any other proc. specified.

        Either loc or df must be set, but they both cannot be set!

    col_name : str, list, optional
        The name(s) of the column(s) to load.

        Note: Must be in the same order as data types passed in.
        (default = None)

    data_type : {'b', 'c', 'm', 'o', 'f'}, optional
        The data types of the different columns to load,
        in the same order as the column names passed in.
        Shorthands for datatypes can be used as well.

        - 'binary' or 'b'
            Binary input

        - 'categorical' or 'c'
            Categorical input

        - 'multilabel' or 'm'
            Multilabel categorical input

        - 'float' or 'f'
            Float numerical input

        .. WARNING::
            If 'multilabel' datatype is specified, then the associated col name
            should be a list of columns, and will be assumed to be.
            For example, if loading multiple targets and one is multilabel,
            a nested list should be passed to col_name.

        Datatypes are explained further in Notes.

        (default = None)

    dataset_type :
    subject_id :
    eventname :
    eventname_col :
    overlap_subjects :

    filter_outlier_percent : float, tuple, list of or None, optional
        For float datatypes only.
        A percent of values to exclude from either end of the
        target distribution, provided as either 1 number,
        or a tuple (% from lower, % from higher).
        set `filter_outlier_percent` to None for no filtering.

        A list of values can also be passed in the case that
        multiple col_names / targets are being loaded. In this
        case, the index should correspond. If a list is not passed
        then the same value is used for all targets.

        (default = None).

    filter_outlier_std : int, float, tuple, None or list of, optional
        For float datatypes only.
        Determines outliers as data points within each column
        (target distribution) where their
        value is less than the mean of the column - `filter_outlier_std[0]`
        * the standard deviation of the column,
        and greater than the mean of the column + `filter_outlier_std[1]`
        * the standard deviation of the column.

        If a single number is passed, that number is applied to both the lower
        and upper range.  If a tuple with None on one side is passed, e.g.
        (None, 3), then nothing will be taken off that lower or upper bound.

        A list of values can also be passed in the case that
        multiple col_names / covars are being loaded. In this
        case, the index should correspond. If a list is not passed
        here, then the same value is used when loading all targets.

        (default = None)

    categorical_drop_percent: float, list of or None, optional
        Optional percentage threshold for dropping categories when
        loading categorical data. If a float is given, then a category
        will be dropped if it makes up less than that % of the data points.
        E.g. if .01 is passed, then any datapoints with a category with less
        then 1% of total valid datapoints is dropped.

        A list of values can also be passed in the case that
        multiple col_names / targets are being loaded. In this
        case, the index should correspond. If a list is not passed
        then the same value is used for all targets.

        (default = None)

    na_values :

    clear_existing : bool, optional
        If this parameter is set to True, then any existing
        loaded targets will first be cleared before loading new targets!

        .. WARNING::
            If any subjects have been dropped from a different place,
            e.g. covars or data, then simply reloading / clearing existing
            covars might result in computing a misleading overlap of final
            valid subjects. Reloading should therefore be best used
            right after loading the original data, or if not possible,
            then reloading the notebook or re-running the script.

        (default = False)

    Notes
    ----------
    Targets can be either 'binary', 'categorical', 'multilabel',
    or 'float',

    - binary
        Targets are read in and label encoded to be 0 or 1,
        Will also work if passed column of unique string also,
        e.g. 'M' and 'F'.

    - categorical
        Targets are treated as taking on one fixed value from a
        limited set of possible values.

    - multilabel
        Simmilar to categorical, but targets can take on multiple
        values, and therefore cannot be reduced to an ordinal representation.

    - float
        Targets are read in as a floating point number,
        and optionally then filtered.
    '''

    if clear_existing:
        self.Clear_Targets()

    # Get the common load params as a mix of user-passed + default values
    load_params = self._make_load_params(args=locals())

    # For targets, set these regardless
    load_params['drop_na'] = True
    load_params['drop_or_na'] = 'drop'

    # Load in the targets w/ basic pre-processing
    targets, col_names = self._common_load(loc, df, dataset_type,
                                           load_params,
                                           col_names=col_name)

    # Proccess the passed in data_types - get right number and in list
    data_types, col_names = proc_datatypes(data_type, col_names)

    # Process pass in other args to be list of len(datatypes)
    fops = proc_args(filter_outlier_percent, data_types)
    foss = proc_args(filter_outlier_std, data_types)
    cdps = proc_args(categorical_drop_percent, data_types)

    # Get drop val, no option for keeping NaN for targets
    drop_val = get_unused_drop_val(targets)

    self._print()

    # Process each target to load
    for key, d_type, fop, fos, cdp in zip(col_names, data_types,
                                          fops, foss, cdps):
        targets =\
            self._proc_target(targets, key, d_type, fop, fos, cdp, drop_val)

    self._print()

    # Drop rows set to drop
    targets = self._drop_from_filter(targets, drop_val)

    self._print('Final shape: ', targets.shape)

    # Merge with existing and set self.targets
    self.targets = self._merge_existing(self.* the standard deviation of the column.

        If a single number is passed, that number is applied to both the lower
        and upper range.  If a tuple with None on one side is passed, e.g.
        (None, 3), then nothing will be taken off that lower or upper bound.

        A list of values can also be passed in the case that
        multiple col_names / covars are being loaded. In this
        case, the index should correspond. If a list is not passed
        here, then the same value is used when loading all targets.

        (default = None)

    categorical_drop_percent: float, list of or None, optional
        Optional percentage threshold for dropping categories when
        loading categorical data. If a float is given, then a category
        will be dropped if it makes up less than that % of the data points.
        E.g. if .01 is passed, then any datapoints with a category with less
        then 1% of total valid datapoints is dropped.

        A list of values can also be passed in the case that
        multiple col_names / targets are being loaded. In this
        case, the index should correspond. If a list is not passed
        then the same value is used for all targets.

        (default = None)

    na_values :

    clear_existing : bool, optional
        If this parameter is set to True, then any existing
        loaded targets will first be cleared before loading new targets!

        .. WARNING::
            If any subjects have been dropped from a different place,
            e.g. covars or data, then simply reloading / clearing existing
            covars might result in computing a misleading overlap of final
            valid subjects. Reloading should therefore be best used
            right after loading the original data, or if not possible,
            then reloading the notebook or re-running the script.

        (default = False)

    Notes
    ----------
    Targets can be either 'binary', 'categorical', 'multilabel',
    or 'float',

    - binary
        Targets are read in and label encoded to be 0 or 1,
        Will also work if passed column of unique string also,
        e.g. 'M' and 'F'.

    - categorical
        Targets are treated as taking on one fixed value from a
        limited set of possible values.

    - multilabel
        Simmilar to categorical, but targets can take on multiple
        values, and therefore cannot be reduced to an ordinal representation.

    - float
        Targets are read in as a floating point number,
        and optionally then filtered.
    '''

    if clear_existing:
        self.Clear_Targets()

    # Get the common load params as a mix of user-passed + default values
    load_params = self._make_load_params(args=locals())

    # For targets, set these regardless
    load_params['drop_na'] = True
    load_params['drop_or_na'] = 'drop'

    # Load in the targets w/ basic pre-processing
    targets, col_names = self._common_load(loc, df, dataset_type,
                                           load_params,
                                           col_names=col_name)

    # Proccess the passed in data_types - get right number and in list
    data_types, col_names = proc_datatypes(data_type, col_names)

    # Process pass in other args to be list of len(datatypes)
    fops = proc_args(filter_outlier_percent, data_types)
    foss = proc_args(filter_outlier_std, data_types)
    cdps = proc_args(categorical_drop_percent, data_types)

    # Get drop val, no option for keeping NaN for targets
    drop_val = get_unused_drop_val(targets)

    self._print()

    # Process each target to load
    for key, d_type, fop, fos, cdp in zip(col_names, data_types,
                                          fops, foss, cdps):
        targets =\
            self._proc_target(targets, key, d_type, fop, fos, cdp, drop_val)

    self._print()

    # Drop rows set to drop
    targets = self._drop_from_filter(targets, drop_val)

    self._print('Final shape: ', targets.shape)

    # Merge with existing and set self.targets
    self.targets = self._merge_existing(self.