{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to show an example with categorical classification, and tries to show some of the nuances . The dataloading and other aspects of the library are brushed over in this notebook without much explanation. Check another for better explanations.\n",
    "\n",
    "The categorical problem we load is demo_relig_v2, which has a number of problems, the most severe being a very dominant single class and a lot of classes with very little representation - in actual practice, this notebook does not represent how we would actually approach a problem like this. This notebook is simply to display categorical capabilities of the library (In practice, a good choice for a problem like this might be to treat it as binary, with the major class vs anything but...)\n",
    "\n",
    "We also only load the first half of the structural data, again just as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ABCD_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nda_dr = '/mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/'\n",
    "test_mapping_loc = nda_dr + 'ABCD_Release_ Notes_Data_Release_ 2.0/22. ABCD_Release_2.0_mapping_r.csv'\n",
    "test_data_loc = nda_dr + 'MRI/ABCD sMRI Part 1.csv'\n",
    "test_target_loc = nda_dr + 'Mental Health/ABCD Parent Demographics Survey.csv'\n",
    "test_exclusion_loc = '/home/sage/bader_things/invalid_pguids.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD_ML object initialized\n",
      "Loading /mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/ABCD_Release_ Notes_Data_Release_ 2.0/22. ABCD_Release_2.0_mapping_r.csv assumed to be dataset type: explorer\n",
      "Loaded map file\n",
      "Loading /mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/MRI/ABCD sMRI Part 1.csv assumed to be dataset type: explorer\n",
      "dropped ['abcd_smrip101_id', 'dataset_id', 'smri_visitid'] columns by default due to dataset type\n",
      "Dropped 0 columns, per drop_keys argument\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 522 rows for missing values\n",
      "Dropped rows with missing data\n",
      "Filtered data for outliers with value:  0.0005\n",
      "Winsorized data with value:  0.001\n",
      "loaded shape:  (8725, 749)\n",
      "\n",
      "Total valid (overlapping subjects / not in exclusions) subjects = 8725\n",
      "\n",
      "Loading targets!\n",
      "Loading /mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/Mental Health/ABCD Parent Demographics Survey.csv assumed to be dataset type: explorer\n",
      "Encoded to 17 categories\n",
      "Final shape:  (11310, 17)\n",
      "\n",
      "Total valid (overlapping subjects / not in exclusions) subjects = 8336\n",
      "\n",
      "Total excluded subjects:  1137\n",
      "\n",
      "Total valid (overlapping subjects / not in exclusions) subjects = 7547\n",
      "\n",
      "Removing non overlapping + excluded subjects\n"
     ]
    }
   ],
   "source": [
    "ML = ABCD_ML.ABCD_ML(default_dataset_type='explorer')\n",
    "\n",
    "ML.Load_Name_Map(loc = test_mapping_loc,\n",
    "                 source_name_col = \"NDAR name\",\n",
    "                 target_name_col = \"REDCap name/NDA alias\")\n",
    "\n",
    "ML.Load_Data(loc=test_data_loc,\n",
    "             dataset_type='explorer',\n",
    "             filter_outlier_percent=.0005, \n",
    "             winsorize_val=.001)\n",
    "\n",
    "ML.Load_Targets(loc=test_target_loc,\n",
    "                col_name='demo_relig_v2',\n",
    "                data_type='c')\n",
    "ML.Load_Exclusions(loc=test_exclusion_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV defined with stratifying behavior, over 17 unique values.\n"
     ]
    }
   ],
   "source": [
    "ML.Define_Validation_Strategy(stratify=ML.original_targets_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are defining our validation strategy as stratifying over the target keys, as some of the religions have very little representation, and we could potentially throw errors if some value is not represented in one of the various validation folds - We still might if we use too much internal CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data for modeling loaded shape: (7547, 766)\n",
      "Performed train/test split, train size: 6037 test size:  1510\n"
     ]
    }
   ],
   "source": [
    "ML.Train_Test_Split(test_size=.2,\n",
    "                    random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No default metric passed, set to, weighted roc auc based on default problem type.\n",
      "No default extra params passed, set to empty dict\n",
      "Default params set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML.Set_Default_ML_Params(problem_type = 'categorical',\n",
    "                         data_scaler = 'standard',\n",
    "                         n_splits = 2,\n",
    "                         n_repeats = 1,\n",
    "                         int_cv = 2,\n",
    "                         class_weight = 'balanced',\n",
    "                         n_jobs = 4,\n",
    "                         n_iter = 2,\n",
    "                         random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: gs and rs are  Grid Search and Random Search\n",
      "Models with gs or rs will have their hyper-parameters tuned accordingly.\n",
      "\n",
      "Problem Type: categorical\n",
      "----------------------\n",
      "Avaliable models: \n",
      "\n",
      "Model str indicator:  dt classifier\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "\n",
      "Model str indicator:  dt classifier gs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "\n",
      "Model str indicator:  elastic net logistic\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      "Model str indicator:  elastic net logistic rs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n",
      "Model str indicator:  gaussian nb\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.naive_bayes.GaussianNB'>\n",
      "\n",
      "Model str indicator:  gp classifier\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.gaussian_process.gpc.GaussianProcessClassifier'>\n",
      "\n",
      "Model str indicator:  knn classifier\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "\n",
      "Model str indicator:  knn classifier gs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "\n",
      "Model str indicator:  lasso logistic\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      "Model str indicator:  lasso logistic rs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n",
      "Model str indicator:  light gbm classifier\n",
      "(MultiClass)\n",
      "Model object:  <class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "\n",
      "Model str indicator:  light gbm classifier rs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n",
      "Model str indicator:  logistic\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      "Model str indicator:  logistic cv\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.linear_model.logistic.LogisticRegressionCV'>\n",
      "\n",
      "Model str indicator:  random forest classifier\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "\n",
      "Model str indicator:  random forest classifier cal\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.calibration.CalibratedClassifierCV'>\n",
      "\n",
      "Model str indicator:  random forest classifier rs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n",
      "Model str indicator:  ridge logistic\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      "Model str indicator:  ridge logistic rs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n",
      "Model str indicator:  svm classifier\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.svm.classes.SVC'>\n",
      "\n",
      "Model str indicator:  svm classifier rs\n",
      "(MultiClass)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n",
      "Model str indicator:  dt classifier\n",
      "(MultiLabel)\n",
      "Model object:  <class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "\n",
      "Model str indicator:  dt classifier gs\n",
      "(MultiLabel)\n",
      "Model object:  <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "\n",
      "Model str indicator:  knn classifier\n",
      "(MultiLabel)\n",
      "Model object:  <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "\n",
      "Model str indicator:  knn classifier gs\n",
      "(MultiLabel)\n",
      "Model object:  <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "\n",
      "Model str indicator:  random forest classifier\n",
      "(MultiLabel)\n",
      "Model object:  <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "\n",
      "Model str indicator:  random forest classifier rs\n",
      "(MultiLabel)\n",
      "Model object:  <class 'sklearn.model_selection._search.RandomizedSearchCV'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML.Show_Model_Types(problem_type='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluate with:\n",
      "model_type = ['knn', 'rf']\n",
      "problem_type = categorical\n",
      "metric = ['macro roc auc', 'macro f1', 'samples roc auc', 'weighted recall', 'log', 'macro jaccard', 'weighted ap']\n",
      "data_scaler = ['standard', 'minmax']\n",
      "n_splits = 2\n",
      "n_repeats = 1\n",
      "int_cv = 2\n",
      "class_weight = balanced\n",
      "n_jobs = 4\n",
      "n_iter = 2\n",
      "random_state = 9\n",
      "extra_params = {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sage/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sage/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric:  macro roc auc\n",
      "Macro mean score:  0.5682030503113262\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.5682030503113262\n",
      "Micro std in score:  0.005678437035310513\n",
      "\n",
      "Metric:  macro f1\n",
      "Macro mean score:  0.01169100501596702\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.01169100501596702\n",
      "Micro std in score:  1.744405403755235e-05\n",
      "\n",
      "Metric:  samples roc auc\n",
      "Macro mean score:  0.8148876200402093\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.8148876200402093\n",
      "Micro std in score:  0.0008680203048471169\n",
      "\n",
      "Metric:  weighted recall\n",
      "Macro mean score:  0.11031994039942832\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.11031994039942832\n",
      "Micro std in score:  0.00016446570541574224\n",
      "\n",
      "Metric:  log\n",
      "Macro mean score:  3.058345971680958\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  3.058345971680958\n",
      "Micro std in score:  0.0830879798636861\n",
      "\n",
      "Metric:  macro jaccard\n",
      "Macro mean score:  0.006490486765711914\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.006490486765711914\n",
      "Micro std in score:  1.0752960181762816e-05\n",
      "\n",
      "Metric:  weighted average precision\n",
      "Macro mean score:  0.19449457363161943\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.19449457363161943\n",
      "Micro std in score:  0.0009278577017673845\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.56252461, 0.01167356, 0.8140196 , 0.11015547, 3.14143395,\n",
       "        0.00647973, 0.19356672],\n",
       "       [0.57388149, 0.01170845, 0.81575564, 0.11048441, 2.97525799,\n",
       "        0.00650124, 0.19542243]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch_of_metrics = ['macro roc auc', 'macro f1', 'samples roc auc', 'weighted recall',\n",
    "                    'log', 'macro jaccard', 'weighted ap']\n",
    "\n",
    "# Ensemble of all multilabel compatible classifiers\n",
    "ML.Evaluate(model_type = ['knn', 'rf'],\n",
    "            metric = bunch_of_metrics,\n",
    "            data_scaler = ['standard', 'minmax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a good example of how samples roc auc can be a terrible choice for categorical problems with big class imbalances.\n",
    "\n",
    "We are also recieving errors about F1 score... this is because within some of our folds the model does not predict any positive samples for some of the classes, which is a direct result of this categorical problem being set up poorly, and that we make no real efforts to explicitly deal with the heavy class imbalance.\n",
    "\n",
    "Of note also is that samples roc auc does not work for multiclass classifiers, which we will show an ensemble of next, so we delete that from from the 'bunch of metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluate with:\n",
      "model_type = ['logistic', 'knn', 'rf']\n",
      "problem_type = categorical\n",
      "metric = ['macro roc auc', 'macro f1', 'weighted recall', 'log', 'macro jaccard', 'weighted ap']\n",
      "data_scaler = ['standard', 'minmax']\n",
      "n_splits = 2\n",
      "n_repeats = 1\n",
      "int_cv = 2\n",
      "class_weight = balanced\n",
      "n_jobs = 4\n",
      "n_iter = 2\n",
      "random_state = 9\n",
      "extra_params = {}\n",
      "\n",
      "Not all model types passed have multilabel support! Using multiclass instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sage/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sage/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/sage/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric:  multiclass macro roc auc\n",
      "Macro mean score:  0.5892976202210078\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.5892976202210078\n",
      "Micro std in score:  0.0014645881841193753\n",
      "\n",
      "Metric:  macro f1\n",
      "Macro mean score:  0.09305446096518839\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.09305446096518839\n",
      "Micro std in score:  8.128611198382485e-05\n",
      "\n",
      "Metric:  weighted recall\n",
      "Macro mean score:  0.23587235749104246\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.23587235749104246\n",
      "Micro std in score:  0.004286425175183142\n",
      "\n",
      "Metric:  log\n",
      "Macro mean score:  2.5245440315301844\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  2.5245440315301844\n",
      "Micro std in score:  0.023857479040590546\n",
      "\n",
      "Metric:  macro jaccard\n",
      "Macro mean score:  0.054250638680474784\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.054250638680474784\n",
      "Micro std in score:  0.00043409052668958586\n",
      "\n",
      "Metric:  multiclass weighted average precision\n",
      "Macro mean score:  0.22836896342782154\n",
      "Macro std in score:  0.0\n",
      "Micro mean score:  0.22836896342782154\n",
      "Micro std in score:  0.002736771426094159\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.59076221, 0.09297317, 0.24015878, 2.50068655, 0.05468473,\n",
       "        0.23110573],\n",
       "       [0.58783303, 0.09313575, 0.23158593, 2.54840151, 0.05381655,\n",
       "        0.22563219]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del bunch_of_metrics[2]\n",
    "\n",
    "\n",
    "ML.Evaluate(model_type = ['logistic', 'knn', 'rf'],\n",
    "                metric = bunch_of_metrics,\n",
    "                data_scaler = ['standard', 'minmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         targets_0    targets_1    targets_2    targets_3    targets_4  \\\n",
      "count  7547.000000  7547.000000  7547.000000  7547.000000  7547.000000   \n",
      "mean      0.110242     0.068372     0.013648     0.195044     0.021068   \n",
      "std       0.313213     0.252399     0.116032     0.396261     0.143620   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "         targets_5    targets_6    targets_7    targets_8    targets_9  \\\n",
      "count  7547.000000  7547.000000  7547.000000  7547.000000  7547.000000   \n",
      "mean      0.095270     0.006095     0.007420     0.003313     0.003048   \n",
      "std       0.293606     0.077838     0.085826     0.057463     0.055124   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "        targets_10   targets_11   targets_12   targets_13   targets_14  \\\n",
      "count  7547.000000  7547.000000  7547.000000  7547.000000  7547.000000   \n",
      "mean      0.007420     0.008878     0.175036     0.022923     0.021200   \n",
      "std       0.085826     0.093809     0.380023     0.149668     0.144062   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "        targets_15   targets_16  \n",
      "count  7547.000000  7547.000000  \n",
      "mean      0.014443     0.226580  \n",
      "std       0.119315     0.418646  \n",
      "min       0.000000     0.000000  \n",
      "25%       0.000000     0.000000  \n",
      "50%       0.000000     0.000000  \n",
      "75%       0.000000     0.000000  \n",
      "max       1.000000     1.000000  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
