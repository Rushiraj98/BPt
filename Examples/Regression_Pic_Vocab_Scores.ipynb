{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this notebook we will look at an example Regression problem. We will try to predict \"nihtbx_picvocab_agecorrected\" scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ABCD_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 2.0.1 release dr, we'll load data from it\n",
    "nda_dr1 = '/mnt/sdb2/ABCDFixRelease2p0p1/'\n",
    "\n",
    "# This is the 2.0 release dr, we'll load targets from it\n",
    "nda_dr2 = '/mnt/sdb2/ABCD2p0NDA/'\n",
    "\n",
    "# We will use the gordon ROI resting state fMRI correlations as our data\n",
    "data_loc = nda_dr1 + 'abcd_betnet02.txt'\n",
    "\n",
    "# This file contains the NIH toolbox scores\n",
    "target_loc = nda_dr2 + 'abcd_tbss01.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of these are default values, but just showing everything as an example,\n",
    "# See the docs / init help string for actual explanation\n",
    "\n",
    "ML = ABCD_ML.ABCD_ML(exp_name = 'Regression_Example',\n",
    "                     log_dr = '',\n",
    "                     existing_log = 'overwrite',\n",
    "                     notebook = True,\n",
    "                     subject_id = 'src_subject_id',\n",
    "                     eventname = 'baseline_year_1_arm_1',\n",
    "                     use_default_subject_ids = True,\n",
    "                     default_dataset_type = 'basic',\n",
    "                     default_na_values = ['777', '999'],\n",
    "                     original_targets_key = 'targets',\n",
    "                     low_memory_mode = False,\n",
    "                     random_state = 1,\n",
    "                     verbose = True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Show_Ensemble_Types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would start by loading exclusions, the reason for this is, if loaded before data or covars, ect... the excluded subjects will be removed when loading data, targets, ect... as loaded before any drop behavior based on values. For example, when computing different filter_outliers, or dropping columns ect... this way it will only consider the non-excluded subjects. \n",
    "\n",
    "We won't load any here, but if there were certain known subjects to exclude they could be read from a file as exclusions or passed in as a list to Load_Exclusions.\n",
    "\n",
    "We will just load in the data instead, specifically we are using the resting state correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Data(loc = data_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a quick look at the data, especially since we have a number of warning columns with not a lot of unique values. This is an indicator that thew column might not be wanted as data is supposed to be neuroimaging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, the obvious problem is that we have a number of columns that we most likely dont want to include. Specifically, there are a number of descriptors, e.g., number of trials, which we don't want, because they are not neuroimaging ROIs. We just want to grab the correlations.\n",
    "Let's clear the data and reload it, explicitly telling the data loader to drop those keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = list(ML.data)[:12]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Clear_Data()\n",
    "ML.Load_Data(loc = data_loc, drop_keys=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, looks better, though the correlations have an extra problem. That is namely, there are repeat columns as X corr with y, and y corr with x. Not, Load_Data has a function to remove duplicate columns (marked as duplicate if they above a user defined correlation threshold with another column). Instead of reloading the data though, we can also call a specific function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Drop_Data_Duplicates(corr_thresh=.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now ets load our targets and some covars (just age + sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Targets(target_loc, 'nihtbx_picvocab_agecorrected', 'float')\n",
    "ML.Load_Covars(target_loc, ['interview_age', 'gender'], ['f', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Show_Targets_Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are some severe outliers... lets try reloading targets and just cut off the top 1% from either side of the distribution and see if that helps. Note: what we are doing here is acting upon the whole dataset before any train/test split, therefore it is reccomended that at this stage any global actions should be reasonable... in this case it seems reasonable to remove subjects with scores that are likely just human input error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Targets(target_loc, 'nihtbx_picvocab_agecorrected', 'float', filter_outlier_percent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Show_Targets_Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One percent might even be too much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Targets(target_loc, 'nihtbx_picvocab_agecorrected', 'float', filter_outlier_percent=.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ML.Show_Targets_Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a little better, so only the really severe outliers are removed. The distribution is still a bit weird, but nothing we can do about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded:\n",
    "-Exclusions\n",
    "-Data, Targets and Covars\n",
    "\n",
    "We could optionally load stratification values, but for this example, we will just use random cross validation.\n",
    "\n",
    "# Validation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the train test split, we would optionally define a validation strategy, but for this expiriment we are just going to use random splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Train_Test_Split(test_size=.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Set_Default_ML_Params(problem_type='regression',\n",
    "                         metric=['r2', 'mse'],\n",
    "                         scaler='standard',\n",
    "                         n_splits=2,\n",
    "                         n_repeats=1,\n",
    "                         int_cv=2,\n",
    "                         n_jobs=8,\n",
    "                         n_iter=20,\n",
    "                         random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'knn regressor',\n",
    "                         feat_selector = 'univariate selection',\n",
    "                         calc_shap_feature_importances = False,\n",
    "                         model_type_param_ind = 0,\n",
    "                         feat_selector_param_ind = 0,\n",
    "                         search_type = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what the above statements are telling us is that we are not using and feature selectors, and then that our default search type is set to None, which means we are by default not doing any sort of search for hyperparameters. In general the way a search for hyperparameters works, is that you can set a param ind, for data scaler, feat_selector and model_type, where the param ind specifies what grid of parameters you want to search over for that object. By default, each object (feat_selector, model type, scaler) has a base grid of parameters consisting of just 1 value, and that is always index 0. So when we set the search type to None, it actually forces every scaler_param_ind and model_type_param_ind to be 0. Are other options are to select search type ='s either 'grid' for an explicit grid search over all param options, or 'random', to search over whatever our value for n_iter number of parameters. Then, you can set any of the param inds to either a different numerical index, specifying a different distribution of params, or each param distribution also has a str indicator name which can be passed.\n",
    "\n",
    "The extra complexity of all of this is worth it I promise, as it lets you define a random search of parameters over not just your model, but optionally a data scaler and feature selector, all relatively easily!\n",
    "\n",
    "Oh but your asking, how do we see these different param ind options for each model or scaler?\n",
    "Heck, I'll show ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets look at just which models are avaliable\n",
    "ML.Show_Models(problem_type='regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now lets choose to look at just the \"svm regressor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Show_Models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the SVR it looks like we just have one other option besides 0, which is a random search option.\n",
    "\n",
    "We can also check for the standard data scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Show_scalers(scaler='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'svm',\n",
    "                         model_type_param_ind = 1,\n",
    "                         search_type = 'random',\n",
    "                         feat_selector = 'rfe',\n",
    "                         n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well no options for that, and we are not using any feature selection, so lets just use param_ind = 1 for the svm regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'svm',\n",
    "                         model_type_param_ind = 1,\n",
    "                         search_type = 'random',\n",
    "                         n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, lets say we wanted to use an elastic net now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'elastic',\n",
    "                         search_type= 'random',\n",
    "                         model_type_param_ind = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'elastic',\n",
    "                         search_type= 'random',\n",
    "                         model_type_param_ind = 1,\n",
    "                         n_iter = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about light gbm you say? That sounds fancy. (For both elastic net and this we just use model_type_param_ind = 1, since it is typically the random search grid of parameters, but definetly look to make sure what you are actuall running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type = 'light gbm',\n",
    "                         search_type='random',\n",
    "                         model_type_param_ind = 1,\n",
    "                         int_cv=2,\n",
    "                         n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
