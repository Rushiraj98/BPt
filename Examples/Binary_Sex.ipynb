{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through a simple binary classification example, explaining general library functionality and data loading along the way.\n",
    "\n",
    "We perform binary classification on sex, using structural MRI rois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ABCD_ML.ABCD_ML import ABCD_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace these values below with your own directory / desired files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define directory with the 2.0_NDA_Data\n",
    "nda_dr = '/mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/'\n",
    "\n",
    "#This file stores the name mapping\n",
    "test_mapping_loc = nda_dr + 'ABCD_Release_ Notes_Data_Release_ 2.0/22. ABCD_Release_2.0_mapping_r.csv'\n",
    "\n",
    "#We will use as the neuroimaging data just the sMRI data\n",
    "test_data_loc1 = nda_dr + 'MRI/ABCD sMRI Part 1.csv'\n",
    "test_data_loc2 = nda_dr + 'MRI/ABCD sMRI Part 2.csv'\n",
    "\n",
    "#We will load target data (and covariate data) from here\n",
    "test_target_loc = nda_dr + 'Mental Health/ABCD Parent Demographics Survey.csv'\n",
    "\n",
    "#We will load stratification data from here\n",
    "test_strat_loc = nda_dr + 'Other Non-Imaging/ABCD ACS Post Stratification Weights.csv'\n",
    "\n",
    "#We will load exclusions from here, it is the list of flipped subject ids\n",
    "test_exclusion_loc = '/home/sage/bader_things/invalid_pguids.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define the class object, which we will use to load load and to train/test different ML models.\n",
    "There are a few global parameters which we can optionally set when defining this object as well, lets look and see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ABCD_ML.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the default parameters are okay for this simple example, but any of them can be changed.\n",
    "\n",
    "One thing we want to change just to make things easier is setting the default_dataset_type field, as in this example all of the datasets we are loading from are 'explorer' type. This way we won't have to pass that to every loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML = ABCD_ML(default_dataset_type='explorer', existing_log='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue by optionally loading in a name map, which is simply a dictionary that attempts to rename any column names loaded in, if those column names are a key in the dictionary. This is useful for ABCD data as the default column names might not be useful.\n",
    "\n",
    "Note this name map and these parameters are for the 'ABCD 2.0 Explorer' formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Name_Map(loc = test_mapping_loc,\n",
    "                 source_name_col = \"NDAR name\",\n",
    "                 target_name_col = \"REDCap name/NDA alias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at what exactly is in this dictionary if we want to confirm we loaded it correctly.\n",
    "It is loaded as name_map within the ABCD_ML class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some_examples = {k: ML.name_map[k] for k in list(ML.name_map)[300:320]}\n",
    "#some_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Load_Exclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our exclusions we will just use the flipped subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Exclusions(loc=test_exclusion_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load in the actual data. Like before we can check what parameters this function wants / can accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Load_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML.Load_Data(loc=[test_data_loc1, test_data_loc2],\n",
    "#             dataset_type='explorer',\n",
    "#             filter_outlier_percent=.005, \n",
    "#             winsorize_val=.01)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That ends up being a lot of data dropped just for dropping missing outliers... since we are not in low_memory_mode, we can just clear the data, and reload it. This time we will also load not just the first data loc, but the rest as well - and at the same time - but just providing the locations of both in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.Clear_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Data(loc=[test_data_loc1, test_data_loc2],\n",
    "             dataset_type='explorer', #This is set as default, but how we would specify it if we wanted to change i\n",
    "             #filter_outlier_percent=.0005,\n",
    "             filter_outlier_percent=.01,\n",
    "             winsorize_val=.001)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem okay settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data for this expiriment should now be loaded. We can check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now that data is loaded we still need to load targets, and can optionally load covars, strat and exclusions. Lets load our target first, and begin as before by checking out the loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Load_Targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, lets just load in sex as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Targets(loc=test_target_loc,\n",
    "                col_name='demo_sex_v2',\n",
    "                data_type='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Show_Targets_Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the verbose print out above, you'll notice that it says \"More than two unique score values found,filtered all but [1. 2.]\" This is because by default when a binary datatype is passed, the dataloader needs to make sure it loads in only two unique values. To solve this when there exists outliers, like in this case, all but the top two unique values by count will be dropped. It will further show which values it has kept, in the case that an error was made, but here 1 and 2 are the correct sex values. If more than two values are desired, the categorical data type should be used.\n",
    "\n",
    "Let's look and see to make sure everything was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look into adding covars next. Where co-variates arn't quite treated as typical co-variates, but are values we would like to be able to pass as additional input to the ML model if desired (and input that is treated in a special way, specifically covar input won't be scaled with any data scaler by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Load_Covars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Covars(loc=test_target_loc,\n",
    "               col_names = 'demo_ed_v2',\n",
    "               data_types = 'o',\n",
    "               standardize = False,\n",
    "               normalize = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check to see it was loaded correctly (and normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.covars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will considering loading different stratification values. These are the values that we can optionally define custom validation / split behavior on. Within this example, we are just going to make sure that all splits preserve subjects with the same family id within the same fold, so lets load family id - after looking as the help function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Load_Strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Load_Strat(loc=test_strat_loc,\n",
    "              col_names='rel_family_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.strat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have our data, targets, covars, strat and exclusions loaded (Noting that the minimum requiriments for running an ML expiriment are just data or covars and targets, the rest being optional). The actual length of the script is also not as terrible as it seems, and once loading behavior is confirmed, verbose can even be turned off. To show this, we can re-load everything as above with verbose off. (Commented out, but you get the idea~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML = ABCD_ML.ABCD_ML(verbose = False) # Reloading the ML object itself to reset everything.\n",
    "\n",
    "#ML.Load_Exclusions(loc=test_exclusion_loc)\n",
    "\n",
    "#ML.Load_Name_Map(loc = test_mapping_loc,\n",
    "#                 source_name_col = \"NDAR name\",\n",
    "#                 target_name_col = \"REDCap name/NDA alias\")\n",
    "\n",
    "#ML.Load_Data(loc=[test_data_loc1, test_data_loc2],\n",
    "#             dataset_type='explorer',\n",
    "#             filter_outlier_percent=.0005, \n",
    "#             winsorize_val=.01)\n",
    "\n",
    "#ML.Load_Targets(loc=test_target_loc,\n",
    "#                col_name='demo_sex_v2',\n",
    "#                data_type='b')\n",
    "\n",
    "#ML.Load_Covars(loc=test_target_loc,\n",
    "#               col_names = 'demo_ed_v2',\n",
    "#               data_types = 'ordinal',\n",
    "#               standardize = False,\n",
    "#               normalize = True)\n",
    "\n",
    "#ML.Load_Strat(loc=test_strat_loc,\n",
    "#              col_names='rel_family_id')\n",
    "\n",
    "#ML.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move onto defining our validation stratagy (which is again optional, but as stated before for this example we are going to preserve family ids within the same folds), if no explicit validation strategy is defined, then random splits will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Define_Validation_Strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for group preserving behavior, as we are interested in keep families within the same folds, we supply an argument for groups. Specifically, we use the name of the column loaded within self.strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Define_Validation_Strategy(groups='rel_family_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly before we get to modelling, we want to define a global train-test split, so that we can perform model exploration, and parameter tuning ect... on a training set, and leave a left-out testing set to eventually test with out final selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Train_Test_Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Train_Test_Split(test_size=.25, #Let be somewhat conservative, and use a size of .25\n",
    "                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - and because we set the validation stratagy to preserve family structure within the folds, we know that no family id is in both the train and test set - for the paranoid we can make sure of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ids = set(ML.strat['rel_family_id'].loc[ML.train_subjects])\n",
    "#test_ids = set(ML.strat['rel_family_id'].loc[ML.test_subjects])\n",
    "\n",
    "#print('Unique family ids in train: ', len(train_ids))\n",
    "#print('Unique family ids in test: ', len(test_ids))\n",
    "#print('Overlap : ', len(train_ids.intersection(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to modeling.\n",
    "\n",
    "The main function we use here is Evaluate, we can look at its docstring, but from a very high level, this is the function we use to test different expirimental setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check what different model types we have avaliable for binary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.Show_Model_Types(problem_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at different metrics avaliable for binary classification, and different data scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.Show_Metrics(problem_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML.Show_Data_Scalers(show_scaler_help=False, show_default_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set some default ML params for some of the settings that we will be keeping the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ML.Set_Default_ML_Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are mostly the same for setting default params as they are passed to Evaluate or Test. Importantly, by defining defaults, we define the value to be used if no value is passed to a given argument in Evaluate or Test.\n",
    "Lets set some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Set_Default_ML_Params(problem_type = 'binary', #This will be staying the same\n",
    "                         metric = ['macro roc auc', 'f1', 'accuracy'], #Lets set a few\n",
    "                         data_scaler = 'standard', #Standard scaling is fine, but this can also be a list\n",
    "                         n_splits = 2, #For the sake of quick run-time and this example 2 splits, no repeats\n",
    "                         n_repeats = 1,\n",
    "                         int_cv = 3,\n",
    "                         class_weight = 'balanced', \n",
    "                         n_jobs = 8,\n",
    "                         n_iter = 8,\n",
    "                         random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some default ML params set, all we need to do to run an evaluation is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='rf',\n",
    "                         sampler='smote',\n",
    "                         ensemble_type = 'basic ensemble',\n",
    "                         ensemble_split = .2,\n",
    "                         n_repeats=1,\n",
    "                         calc_shap_feature_importances = True,\n",
    "                         calc_base_feature_importances = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Plot_Base_Feat_Importances(top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Plot_Shap_Feat_Importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Plot_Shap_Summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcbcvb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try with multiple data scalers now, just to show off that functionality, and still just the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='logistic',\n",
    "             data_scaler = ['robust', 'minmax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robust scaler and minmax scaler seem to help a bit, so hey lets set that as our new default... just for fun.\n",
    "\n",
    "Note when setting a new param, you do not need to repass in everything you are not changing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Set_Default_ML_Params(data_scaler = ['robust', 'minmax'])\n",
    "\n",
    "# Let make just take a look at the new default params, to make sure it is as expected\n",
    "ML.default_ML_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a more complicated classifier, in particular, let's\n",
    "try the light gradient boosting machine (LGBM) classifier, with a random search for parameters.\n",
    "Note: The ' rs' at the end of the str model indicator is a special key for selecting the random parameter search option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='lgbm classifier',\n",
    "                         search_type='random',\n",
    "                         model_type_param_ind=1,\n",
    "                         n_iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the basic logistic regression seems to perform better then the more complex gradient boosting. One thing to note though is that we only tested 8 random sets of hyperparamers, what if we tried exploring a larger number? (This might take a while, especially if you are not running this on a powerful computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='lgbm classifier rs',\n",
    "                         n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is still not all that big a boost from just a logistic regression.\n",
    "We can still explore some other linear classifiers though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='ridge logistic rs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='knn classifier gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='gaussian nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a gaussian process classifier\n",
    "raw_scores = ML.Evaluate(model_type='gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = ML.Evaluate(model_type='elastic net logistic rs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could easily continue with the model exploration portion, but for this example, lets say we have explored enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Test(model_type ='ridge logistic rs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
