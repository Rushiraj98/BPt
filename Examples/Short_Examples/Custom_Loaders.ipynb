{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is designed to explore some interesting features avaliable through ABCD_ML, specifically loading Data_Files and using custom loaders.\n",
    "\n",
    "This example will require some extra optional ABCD_ML libraries, including nibabel and nilearn!\n",
    "\n",
    "We will also use fake data for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ABCD_ML import *\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by saving some fake surface time-series data, and also some fake just surface data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random(size = (20, 100, 10242))\n",
    "os.makedirs('fake_time_data', exist_ok=True)\n",
    "\n",
    "for x in range(len(X)):\n",
    "    np.save('fake_time_data/' + str(x) + '_lh', X[x])\n",
    "for x in range(len(X)):\n",
    "    np.save('fake_time_data/' + str(x) + '_rh', X[x])\n",
    "    \n",
    "X = np.random.random(size = (20, 10242))\n",
    "os.makedirs('fake_surf_data', exist_ok=True)\n",
    "\n",
    "for x in range(len(X)):\n",
    "    np.save('fake_surf_data/' + str(x) + '_lh', X[x])\n",
    "for x in range(len(X)):\n",
    "    np.save('fake_surf_data/' + str(x) + '_rh', X[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this expiriment we will load both the timeseries and the just surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_name = My_ML_Exp\n",
      "log_dr = None\n",
      "existing_log = append\n",
      "verbose = True\n",
      "exp log dr setup at: None\n",
      "log file at: None\n",
      "Default params set:\n",
      "notebook = True\n",
      "use_abcd_subject_ids = False\n",
      "low memory mode = False\n",
      "strat_u_name = _Strat\n",
      "random state = 534\n",
      "n_jobs = 1\n",
      "dpi = 100\n",
      "mp_context = spawn\n",
      "ABCD_ML object initialized\n",
      "Setting default load params, as they have not been set!\n",
      "\n",
      "Default load params set within self.default_load_params.\n",
      "----------------------\n",
      "dataset_type: basic\n",
      "subject_id: src_subject_id\n",
      "eventname: None\n",
      "eventname_col: eventname\n",
      "overlap_subjects: False\n",
      "merge: inner\n",
      "na_values: ['777', '999']\n",
      "drop_na: True\n",
      "drop_or_na: drop\n",
      "\n",
      "To change the default load params, call self.Set_Default_Load_Params()\n",
      "\n",
      "Loading from df or files\n",
      "\n",
      "Dropped 1 columns per passed drop_keys argument\n",
      "Loading from df or files\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 0 rows for missing values, based on the provided drop_na param: True with actual na_thresh: 0\n",
      "Loaded rows with NaN remaining: 0\n",
      "\n",
      "loading: target\n",
      "\n",
      "Loaded Shape: (20, 1)\n",
      "All loaded targets\n",
      "0 : target\n",
      "\n",
      "Loading from df or files\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 0 rows for missing values, based on the provided drop_na param: True with actual na_thresh: 0\n",
      "Loaded rows with NaN remaining: 0\n",
      "Loaded Shape: (20, 1)\n",
      "Calling Prepare_All_Data() to change the default merge behavior call it again!\n",
      "Preparing final data, in self.all_data\n",
      "Any changes to loaded data, covars or strat will not be included, from now on.\n",
      "\n",
      "Final data (w/ target) for modeling loaded shape: (20, 6)\n",
      "Performing split on 20 subjectsWarning: Test size of 0 passed, all subjects set to train.\n",
      "\n",
      "Performed train test split\n",
      "Train size: 20\n",
      "Test size:  0\n"
     ]
    }
   ],
   "source": [
    "ML = ABCD_ML(log_dr=None, verbose=True)\n",
    "\n",
    "timeseries_dr = 'fake_time_data/'\n",
    "files = os.listdir(timeseries_dr)\n",
    "lh_timeseries = [timeseries_dr + f for f in files if '_lh' in f]\n",
    "rh_timeseries = [timeseries_dr + f for f in files if '_rh' in f]\n",
    "\n",
    "surf_dr = 'fake_surf_data/'\n",
    "files = os.listdir(surf_dr)\n",
    "lh_surf = [surf_dr + f for f in files if '_lh' in f]\n",
    "rh_surf = [surf_dr + f for f in files if '_rh' in f]\n",
    "\n",
    "subjects = [str(i) for i in range(20)]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['lh_timeseries'] = lh_timeseries\n",
    "df['rh_timeseries'] = rh_timeseries\n",
    "\n",
    "df['lh_surf'] = lh_surf\n",
    "df['rh_surf'] = rh_surf\n",
    "\n",
    "df['src_subject_id'] = subjects\n",
    "df['target'] = np.random.randint(2, size=20)\n",
    "\n",
    "\n",
    "ML.Load_Data_Files(df = df,\n",
    "                   load_func = np.load,\n",
    "                   drop_keys = ['target'])\n",
    "\n",
    "ML.Load_Targets(df = df, col_name='target', data_type='b')\n",
    "ML.Load_Strat(df = df, col_name='target')\n",
    "\n",
    "ML.Train_Test_Split(test_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will assume you have some saved parcellations in the relevant space, i.e., we saved fake fsaverage5 surface data, so we will load in the desikan parcellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ABCD_ML.extensions import SurfLabels, Connectivity, Networks\n",
    "\n",
    "base = '/home/sage/work/Parcel_Search/Existing_Parcels/'\n",
    "desikan_lh = base + 'lh.aparc.annot'\n",
    "desikan_rh = base + 'rh.aparc.annot'\n",
    "\n",
    "t_surf_rois_lh = SurfLabels(labels = desikan_lh,\n",
    "                            vectorize = False)\n",
    "t_surf_rois_rh = SurfLabels(labels = desikan_rh,\n",
    "                            vectorize = False)\n",
    "\n",
    "connectivity = Connectivity(vectorize=True, discard_diagonal=False)\n",
    "\n",
    "surf_rois_lh = SurfLabels(labels = desikan_lh)\n",
    "surf_rois_rh = SurfLabels(labels = desikan_rh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try just loading the left hemisphere surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_spec = Problem_Spec(problem_type = 'binary',\n",
    "                            scope = 'lh_surf')\n",
    "\n",
    "loaders = Loader(surf_rois_lh, scope='lh_surf')\n",
    "\n",
    "model_pipeline = Model_Pipeline(loaders=loaders,\n",
    "                                feat_selectors=Feat_Selector('univariate selection'),\n",
    "                                feat_importances=Feat_Importance('shap'))\n",
    "\n",
    "results = ML.Evaluate(model_pipeline, problem_spec)\n",
    "\n",
    "for step in ML.Model_Pipeline.Model.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['FIs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Plot_Global_Feat_Importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_spec = Problem_Spec(problem_type = 'binary',\n",
    "                            scope = '_surf')\n",
    "\n",
    "loaders = [Loader(surf_rois_lh, scope='lh_surf'),\n",
    "           Loader(surf_rois_rh, scope='rh_surf')]\n",
    "\n",
    "model_pipeline = Model_Pipeline(loaders=loaders,\n",
    "                                feat_importances=Feat_Importance('shap'))\n",
    "\n",
    "results = ML.Evaluate(model_pipeline, problem_spec)\n",
    "\n",
    "for step in ML.Model_Pipeline.Model.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['FIs'][0].inverse_global_fis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_spec.scope = 'all'\n",
    "\n",
    "loaders = [Loader(surf_rois_lh, scope='lh_surf'),\n",
    "           Loader(surf_rois_rh, scope='rh_surf'),\n",
    "           Loader(Pipe([t_surf_rois_lh, connectivity]), scope='lh_timeseries'),\n",
    "           Loader(Pipe([t_surf_rois_rh, connectivity]), scope='rh_timeseries')]\n",
    "\n",
    "model_pipeline = Model_Pipeline(loaders=loaders)\n",
    "results = ML.Evaluate(model_pipeline, problem_spec)\n",
    "\n",
    "for step in ML.Model_Pipeline.Model.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_spec.scope = 'timeseries'\n",
    "\n",
    "loaders = [Loader(surf_rois_lh, scope='lh_timeseries'),\n",
    "           Loader(surf_rois_rh, scope='rh_timeseries')]\n",
    "\n",
    "model_pipeline = Model_Pipeline(loaders=loaders)\n",
    "results = ML.Evaluate(model_pipeline, problem_spec)\n",
    "\n",
    "for step in ML.Model_Pipeline.Model.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Networks class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ABCD_ML.extensions import SurfLabels, Connectivity, Networks\n",
    "def my_load_func(loc):\n",
    "    data = pd.read_csv(loc, sep='\\t', header=None)\n",
    "    data=data.drop(data.columns[0], axis=1)\n",
    "    return np.array(data)\n",
    "\n",
    "data_loc = '/home/sage/Downloads/TS/'\n",
    "\n",
    "def file_to_subject_func(file):\n",
    "    subject = file.split('/')[-1].split('_')[0]\n",
    "    return subject\n",
    "\n",
    "ML = ABCD_ML(log_dr=None, verbose=False)\n",
    "\n",
    "files = {'run1': [os.path.join(data_loc, f) for f in os.listdir(data_loc) if '_01.txt' in f]}\n",
    "\n",
    "file_to_subject = file_to_subject_func\n",
    "\n",
    "ML.Load_Data_Files(files = files,\n",
    "                   file_to_subject = file_to_subject,\n",
    "                   clear_existing=True,\n",
    "                   load_func=my_load_func)\n",
    "\n",
    "copy = ML.data.copy()\n",
    "copy['target'] = np.random.random(len(ML.data))\n",
    "\n",
    "ML.Load_Targets(df=copy,\n",
    "                col_name=['target'],\n",
    "                data_type='f',\n",
    "                clear_existing=True)\n",
    "\n",
    "ML.Train_Test_Split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeeeddd5ee504795a7b2952369f62649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repeats', max=1.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60367faa2808440183cb60584d512c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Folds', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "problem_spec = Problem_Spec(problem_type = 'regression',\n",
    "                            n_jobs=16, random_state=2)\n",
    "\n",
    "connectivity = Connectivity(vectorize=False, kind='correlation', discard_diagonal=True)\n",
    "\n",
    "nets = Networks(threshold=.1,\n",
    "                threshold_method='density',\n",
    "                to_compute=['avg_degree','avg_cluster'])\n",
    "\n",
    "\n",
    "#loaders = Select(loader_list)\n",
    "\n",
    "nets_params= {'threshold':ng.p.Choice([.2,.3])}\n",
    "\n",
    "loader4 = Loader(obj = Pipe([connectivity, nets]),\n",
    "                 params = [0, 0])\n",
    "\n",
    "#loaders = [Loader(Pipe([connectivity,reshape,nets]))]\n",
    "\n",
    "dt= Model('dt regressor')\n",
    "\n",
    "model_pipeline = Model_Pipeline(loaders=loader4,\n",
    "                                scalers=None,\n",
    "                                model=dt)\n",
    "                               \n",
    "\n",
    "results = ML.Evaluate(model_pipeline, problem_spec, n_repeats=1, splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "cloned_transformer = clone(ML.Model_Pipeline.Model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = cloned_transformer.file_mapping.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(cloned_transformer, X, y):\n",
    "    \n",
    "    qq = cloned_transformer.fit_transform(X, y)\n",
    "    \n",
    "    return qq, cloned_transformer\n",
    "\n",
    "test_cached = memory.cache(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cached._get_argument_hash(cloned_transformer.file_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib.hashing import hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [q[0], q[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [q[1], q[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(r), hash(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cached(cloned_transformer, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = check_memory('/home/sage/temp')\n",
    "#fit_transform_one_cached = memory.cache(_fit_transform_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ML.all_data['run1'])\n",
    "y = np.array(ML.all_data['target'])\n",
    "\n",
    "X = X.reshape((52, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, fitted_transformer = fit_transform_one_cached(\n",
    "                cloned_transformer, X, y, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import _print_elapsed_time\n",
    "from sklearn.base import clone\n",
    "from distutils.version import LooseVersion\n",
    "import joblib\n",
    "\n",
    "def _fit_transform_one(transformer,\n",
    "                       X,\n",
    "                       y,\n",
    "                       weight,\n",
    "                       message_clsname='',\n",
    "                       message=None,\n",
    "                       **fit_params):\n",
    "    \"\"\"\n",
    "    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n",
    "    with the fitted transformer. If ``weight`` is not ``None``, the result will\n",
    "    be multiplied by ``weight``.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.utils import _print_elapsed_time\n",
    "    \n",
    "    with _print_elapsed_time(message_clsname, message):\n",
    "        if hasattr(transformer, 'fit_transform'):\n",
    "            res = transformer.fit_transform(X, y, **fit_params)\n",
    "        else:\n",
    "            res = transformer.fit(X, y, **fit_params).transform(X)\n",
    "\n",
    "    if weight is None:\n",
    "        return res, transformer\n",
    "    return res * weight, transformer\n",
    "\n",
    "def _fit_transform_one2(transformer,\n",
    "                       X,\n",
    "                       y,\n",
    "                       weight,\n",
    "                       message_clsname='',\n",
    "                       message=None,\n",
    "                       **fit_params):\n",
    "    \n",
    "\n",
    "    if hasattr(transformer, 'fit_transform'):\n",
    "        res = transformer.fit_transform(X, y, **fit_params)\n",
    "    else:\n",
    "        res = transformer.fit(X, y, **fit_params).transform(X)\n",
    "\n",
    "    if weight is None:\n",
    "        return res, transformer\n",
    "    return res * weight, transformer\n",
    "\n",
    "\n",
    "def check_memory(memory):\n",
    "    \"\"\"Check that ``memory`` is joblib.Memory-like.\n",
    "    joblib.Memory-like means that ``memory`` can be converted into a\n",
    "    joblib.Memory instance (typically a str denoting the ``location``)\n",
    "    or has the same interface (has a ``cache`` method).\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory : None, str or object with the joblib.Memory interface\n",
    "    Returns\n",
    "    -------\n",
    "    memory : object with the joblib.Memory interface\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If ``memory`` is not joblib.Memory-like.\n",
    "    \"\"\"\n",
    "\n",
    "    if memory is None or isinstance(memory, str):\n",
    "        if LooseVersion(joblib.__version__) < '0.12':\n",
    "            memory = joblib.Memory(cachedir=memory, verbose=10)\n",
    "        else:\n",
    "            memory = joblib.Memory(location=memory, verbose=10)\n",
    "    elif not hasattr(memory, 'cache'):\n",
    "        raise ValueError(\"'memory' should be None, a string or have the same\"\n",
    "                         \" interface as joblib.Memory.\"\n",
    "                         \" Got memory='{}' instead.\".format(memory))\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('home': conda)",
   "language": "python",
   "name": "python37664bithomeconda2aade2e1d0ce4797afe91f4891a59d68"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
