{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through a simple binary classification example, explaining library functionality along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ABCD_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define directory with the 2.0_NDA_Data\n",
    "nda_dr = '/mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/'\n",
    "\n",
    "#This file stores the name mapping\n",
    "test_mapping_loc = nda_dr + 'ABCD_Release_ Notes_Data_Release_ 2.0/22. ABCD_Release_2.0_mapping_r.csv'\n",
    "\n",
    "#We will use as the neuroimaging data just the sMRI data\n",
    "test_data_loc1 = nda_dr + 'MRI/ABCD sMRI Part 1.csv'\n",
    "test_data_loc2 = nda_dr + 'MRI/ABCD sMRI Part 2.csv'\n",
    "\n",
    "#We will load target data (and covariate data) from here\n",
    "test_target_loc = nda_dr + 'Mental Health/ABCD Parent Demographics Survey.csv'\n",
    "\n",
    "#We will load stratification data from here\n",
    "test_strat_loc = nda_dr + 'Other Non-Imaging/ABCD ACS Post Stratification Weights.csv'\n",
    "\n",
    "#We will load exclusions from here, it is the list of flipped subject ids\n",
    "test_exclusion_loc = '/home/sage/bader_things/invalid_pguids.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define the class object, which we will use to load load and to train/test different ML models.\n",
    "There are a few global parameters which we can optionally set when defining this object as well, lets look and see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module ABCD_ML.ABCD_ML:\n",
      "\n",
      "__init__(self, eventname='baseline_year_1_arm_1', use_default_subject_ids=True, default_na_values=['777', '999'], n_jobs=1, original_targets_key='targets', low_memory_mode=False, verbose=True)\n",
      "    Main class init\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    eventname : str or None, optional\n",
      "        Optional value to provide, specifying to keep certain rows\n",
      "        when reading data based on the eventname flag.\n",
      "        As ABCD is a longitudinal study, this flag lets you select only\n",
      "        one specific time point, or if set to None, will load everything.\n",
      "        (default = baseline_year_1_arm_1)\n",
      "    \n",
      "    use_default_subject_ids : bool, optional\n",
      "        Flag to determine the usage of 'default' subject id behavior.\n",
      "        If set to True, this will convert input NDAR subject ids\n",
      "        into upper case, with prepended NDAR_ - type format.\n",
      "        If set to False, then all input subject names must be entered\n",
      "        explicitly the same, no preprocessing will be done on them.\n",
      "        (default = True)\n",
      "    \n",
      "    default_na_values : list, optional\n",
      "        Additional values to treat as NaN, by default ABCD specific\n",
      "        values of '777' and '999' are treated as NaN,\n",
      "        and those set to default by pandas 'read_csv' function.\n",
      "        Note: if new values are passed here,\n",
      "        it will override these default '777' and '999' NaN values.\n",
      "        (default = ['777', '999'])\n",
      "    \n",
      "    n_jobs : int, optional\n",
      "        Number of processors to use during training\n",
      "        of machine learning models. This default parameter can\n",
      "        still be overriden if n_jobs is passed in\n",
      "        extra params in a specific training instance.\n",
      "        (default = 1)\n",
      "    \n",
      "    original_targets_key : str, optional\n",
      "        This parameter refers to the column name / key, that the\n",
      "        target variable of interest will be stored under. There are not a\n",
      "        lot of reasons to change this setting, except in the case of\n",
      "        a naming conflict - or just for further customization.\n",
      "        (default = 'targets')\n",
      "    \n",
      "    low_memory_mode : bool, optional\n",
      "        This parameter dictates behavior around loading in data,\n",
      "        specifically, if `low_memory_mode` is set to True,\n",
      "        then when loading data from multiple sources, only common\n",
      "        subjects will be saved as each data source is loaded.\n",
      "        For comparison, when low memory mode if off, the dropping\n",
      "        of non-common subjects occurs later. Though regardless of if low\n",
      "        memory mode is on or off, subjects will be dropped right away\n",
      "        when exclusions or strat is loaded. Non low memory mode\n",
      "        behavior is useful when the user wants to try loading different\n",
      "        data, and doesn't want automatic drops to occur.\n",
      "        If set to True, individual dataframes self.data, self.covars ect...\n",
      "        will also be deleted from memory as soon as modeling begins.\n",
      "        (default = False)\n",
      "    \n",
      "    verbose: bool, optional\n",
      "        If set to true will display diagnostic and other output during\n",
      "        dataloading and model training ect... if set to False this output\n",
      "        will be muted.\n",
      "        (default = True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ABCD_ML.ABCD_ML.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the default parameters are okay for this simple example, but any of them can be changed. Let's change n_jobs to 4 instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD_ML object initialized\n"
     ]
    }
   ],
   "source": [
    "ML = ABCD_ML.ABCD_ML(n_jobs = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue by optionally loading in a name map, which is simply a dictionary that attempts to rename any column names loaded in, if those column names are a key in the dictionary. This is useful for ABCD data as the default column names might not be useful.\n",
    "\n",
    "Note this name map and these parameters are for the 'ABCD 2.0 Explorer' formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded map file\n"
     ]
    }
   ],
   "source": [
    "ML.load_name_map(loc = test_mapping_loc,\n",
    "                 source_name_col = \"NDAR name\",\n",
    "                 target_name_col = \"REDCap name/NDA alias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at what exactly is in this dictionary if we want to confirm we loaded it correctly.\n",
    "It is loaded as name_map within the ABCD_ML class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ddtidp_674': 'dmri_dtifagm_cortdestrieux_ssuborbitallh',\n",
       " 'ddtidp_748': 'dmri_dtifagm_cortdestrieux_ssuborbitalrh',\n",
       " 'ddtidp_675': 'dmri_dtifagm_cortdestrieux_ssubparietallh',\n",
       " 'ddtidp_749': 'dmri_dtifagm_cortdestrieux_ssubparietalrh',\n",
       " 'ddtidp_676': 'dmri_dtifagm_cortdestrieux_stemporalinflh',\n",
       " 'ddtidp_750': 'dmri_dtifagm_cortdestrieux_stemporalinfrh',\n",
       " 'ddtidp_677': 'dmri_dtifagm_cortdestrieux_stemporalsuplh',\n",
       " 'ddtidp_751': 'dmri_dtifagm_cortdestrieux_stemporalsuprh',\n",
       " 'ddtidp_678': 'dmri_dtifagm_cortdestrieux_stemporaltransverselh',\n",
       " 'ddtidp_752': 'dmri_dtifagm_cortdestrieux_stemporaltransverserh',\n",
       " 'dmri_dtifagwc_cdsn_bslh': 'dmri_dtifagwc_cortdesikan_banksstslh',\n",
       " 'dmri_dtifagwc_cdsn_bsrh': 'dmri_dtifagwc_cortdesikan_banksstsrh',\n",
       " 'dmri_dtifagwc_cdsn_cdacatelh': 'dmri_dtifagwc_cortdesikan_caudalanteriorcingulatelh',\n",
       " 'dmri_dtifagwc_cdsn_cdacaterh': 'dmri_dtifagwc_cortdesikan_caudalanteriorcingulaterh',\n",
       " 'dmri_dtifagwc_cdsn_cdmflh': 'dmri_dtifagwc_cortdesikan_caudalmiddlefrontallh',\n",
       " 'dmri_dtifagwc_cdsn_cdmfrh': 'dmri_dtifagwc_cortdesikan_caudalmiddlefrontalrh',\n",
       " 'dmri_dtifagwc_cdsn_cuneuslh': 'dmri_dtifagwc_cortdesikan_cuneuslh',\n",
       " 'dmri_dtifagwc_cdsn_cuneusrh': 'dmri_dtifagwc_cortdesikan_cuneusrh',\n",
       " 'dmri_dtifagwc_cdsn_ehinallh': 'dmri_dtifagwc_cortdesikan_entorhinallh',\n",
       " 'dmri_dtifagwc_cdsn_ehinalrh': 'dmri_dtifagwc_cortdesikan_entorhinalrh'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_examples = {k: ML.name_map[k] for k in list(ML.name_map)[300:320]}\n",
    "some_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load in the actual data. Like before we can check what parameters this function wants / can accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load_data in module ABCD_ML._Data:\n",
      "\n",
      "load_data(loc, dataset_type, drop_keys=[], filter_outlier_percent=None, winsorize_val=None) method of ABCD_ML.ABCD_ML.ABCD_ML instance\n",
      "    Load a ABCD2p0NDA (default) or 2.0_ABCD_Data_Explorer (explorer)\n",
      "    release formatted neuroimaging dataset - of derived ROI level info.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    loc : str, Path or list of\n",
      "        The location of the csv file to load data load from.\n",
      "        If passed a list, then will load each loc in the list,\n",
      "        and will assume them all to be of the same dataset_type if one\n",
      "        dataset_type is passed, or if they differ in type, a list must be\n",
      "        passed to dataset_type with the different types in order.\n",
      "        Note: some proc will be done on each loaded dataset before merging\n",
      "        with the rest (duplicate subjects, proc for eventname ect...), but\n",
      "        other dataset loading behavior won't occur until after the merge,\n",
      "        e.g., dropping cols by key, filtering for outlier, ect...\n",
      "    \n",
      "    dataset_type : {'default', 'explorer', 'custom'} or list of, optional\n",
      "        The type of dataset to load from. If a list is passed, then loc must\n",
      "        also be a list, and the indices should correspond.\n",
      "        Likewise, if loc is a list and dataset_type is not,\n",
      "        it is assumed all datasets are the same type.\n",
      "        Where each dataset type is,\n",
      "    \n",
      "        - 'default' : ABCD2p0NDA style, (.txt and tab seperated)\n",
      "            The 4 columns before 'src_subject_id' and the 4 after,\n",
      "            (typically the default columns, and therefore not neuroimaging\n",
      "            data - also not including the eventname column), will be dropped.\n",
      "    \n",
      "        - 'explorer' : 2.0_ABCD_Data_Explorer tyle (.csv and comma seperated)\n",
      "            The first 2 columns before 'src_subject_id'\n",
      "            (typically the default columns, and therefore not neuroimaging\n",
      "            data - also not including the eventname column), will be dropped.\n",
      "    \n",
      "        - 'custom' : A user-defined custom dataset. Right now this is only\n",
      "            supported as a comma seperated file, with the subject names in a\n",
      "            column called 'src_subject_id'. No columns will be dropped,\n",
      "            unless specific drop keys are passed.\n",
      "    \n",
      "        (default = 'default')\n",
      "    \n",
      "    drop_keys : list, optional\n",
      "        A list of keys to drop columns by, where if any key given in a columns\n",
      "        name, then that column will be dropped.\n",
      "        (Note: if a name mapping exists, this drop step will be\n",
      "        conducted after renaming)\n",
      "        (default = [])\n",
      "    \n",
      "    filter_outlier_percent : int, float, tuple or None, optional\n",
      "        For float / ordinal data only.\n",
      "        A percent of values to exclude from either end of the\n",
      "        targets distribution, provided as either 1 number,\n",
      "        or a tuple (% from lower, % from higher).\n",
      "        set `filter_outlier_percent` to None for no filtering.\n",
      "        (default = None)\n",
      "        If over 1 then treated as a percent, if under 1, then\n",
      "        used directly.\n",
      "    \n",
      "    winsorize_val : float, tuple or None, optional\n",
      "        The (winsorize_val[0])th lowest values are set to\n",
      "        the (winsorize_val[0])th percentile,\n",
      "        and the (winsorize_val[1])th highest values\n",
      "        are set to the (1 - winsorize_val[1])th percentile.\n",
      "        If one value passed, used for both ends.\n",
      "        If None, then no winsorization performed.\n",
      "        Note: Winsorizing will be performed after\n",
      "        filtering for outliers if values are passed for both.\n",
      "        (default = None)\n",
      "    \n",
      "    Notes\n",
      "    ----------\n",
      "    For loading a truly custom dataset, an advanced user can\n",
      "    load all the data themselves into a pandas DataFrame.\n",
      "    They will need to have the DataFrame indexed by 'src_subject_id'\n",
      "    e.g., data = data.set_index('src_subject_id')\n",
      "    and subject ids will need to be in the correct style...\n",
      "    but if they do all this, then they can just set\n",
      "    self.data = whatever_they_loaded_their_data_as\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ML.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/MRI/ABCD sMRI Part 1.csv assumed to be dataset type: explorer\n",
      "dropped ['abcd_smrip101_id', 'dataset_id', 'smri_visitid'] columns by default due to dataset type\n",
      "Dropped 0 columns, per drop_keys argument\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 522 rows for missing values\n",
      "Dropped rows with missing data\n",
      "Filtered data for outliers with value:  0.005\n",
      "Winsorized data with value:  0.01\n",
      "loaded shape:  (2099, 749)\n",
      "\n",
      "Total valid (overlapping subjects / not in exclusions) subjects = 2099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML.load_data(loc=test_data_loc1,\n",
    "             dataset_type='explorer',\n",
    "             filter_outlier_percent=.005, \n",
    "             winsorize_val=.01)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That ends up being a lot of data dropped just for dropping missing outliers... since we are not in low_memory_mode, we can just clear the data, and reload it. This time we will also load not just the first data loc, but the rest as well - and at the same time - but just providing the locations of both in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleared data.\n"
     ]
    }
   ],
   "source": [
    "ML.clear_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/MRI/ABCD sMRI Part 1.csv assumed to be dataset type: explorer\n",
      "dropped ['abcd_smrip101_id', 'dataset_id', 'smri_visitid'] columns by default due to dataset type\n",
      "Loading /mnt/sdb2/2.0_ABCD_Data_Explorer/2.0_NDA_Data/MRI/ABCD sMRI Part 2.csv assumed to be dataset type: explorer\n"
     ]
    }
   ],
   "source": [
    "ML.load_data(loc=[test_data_loc1, test_data_loc2],\n",
    "             dataset_type='explorer',\n",
    "             filter_outlier_percent=.0005, \n",
    "             winsorize_val=.001)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem okay settings, we can load the next half of the data with these as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data for this expiriment should now be loaded. We can check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now that data is loaded we still need to load targets, and can optionally load covars, strat and exclusions. Lets load our target first, and begin as before by checking out the loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ML.load_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, lets just load in sex as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.load_targets(loc=test_target_loc,\n",
    "                col_name='demo_sex_v2',\n",
    "                data_type='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the verbose print out above, you'll notice that it says \"More than two unique score values found,filtered all but [1. 2.]\" This is because by default when a binary datatype is passed, the dataloader needs to make sure it loads in only two unique values. To solve this when there exists outliers, like in this case, all but the top two unique values by count will be dropped. It will further show which values it has kept, in the case that an error was made, but here 1 and 2 are the correct sex values. If more than two values are desired, the categorical data type should be used.\n",
    "\n",
    "Let's look and see to make sure everything was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look into adding covars next. Where co-variates arn't quite treated as typical co-variates, but are values we would like to be able to pass as additional input to the ML model if desired (and input that is treated in a special way, specifically covar input won't be scaled with any data scaler by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ML.load_covars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.load_covars(loc=test_target_loc,\n",
    "               col_names = 'demo_ed_v2',\n",
    "               data_types = 'ordinal',\n",
    "               standardize = False,\n",
    "               normalize = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check to see it was loaded correctly (and normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.covars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading stratification values (strat), these are the values that we can optionally define custom validation / split behavior on. Within this example, we are just going to make sure that all splits preserve subjects with the same family id within the same fold, so lets load family id - after looking as the help function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ML.load_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.load_strat(loc=test_strat_loc,\n",
    "              col_names='rel_family_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.strat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great looks good. Lastly, we can still optionally load in a list of subject ids to exclude - for whatever reason, from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ML.load_exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.load_exclusions(loc=test_exclusion_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have our data, targets, covars, strat and exclusions loaded (Noting that the minimum requiriments for running an ML expiriment are just data or covars and targets, the rest being optional). The actual length of the script is also not as terrible as it seems, and once loading behavior is confirmed, verbose can even be turned off. To show this, we can re-load everything as above with verbose off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML = ABCD_ML.ABCD_ML(n_jobs = 4, verbose = False) # Reloading the ML object itself to reset everything.\n",
    "\n",
    "ML.load_name_map(loc = test_mapping_loc,\n",
    "                 source_name_col = \"NDAR name\",\n",
    "                 target_name_col = \"REDCap name/NDA alias\")\n",
    "\n",
    "ML.load_data(loc=[test_data_loc1, test_data_loc2],\n",
    "             dataset_type='explorer',\n",
    "             filter_outlier_percent=.0005, \n",
    "             winsorize_val=.01)\n",
    "\n",
    "ML.load_targets(loc=test_target_loc,\n",
    "                col_name='demo_sex_v2',\n",
    "                data_type='b')\n",
    "\n",
    "ML.load_covars(loc=test_target_loc,\n",
    "               col_names = 'demo_ed_v2',\n",
    "               data_types = 'ordinal',\n",
    "               standardize = False,\n",
    "               normalize = True)\n",
    "\n",
    "ML.load_strat(loc=test_strat_loc,\n",
    "              col_names='rel_family_id')\n",
    "\n",
    "ML.load_exclusions(loc=test_exclusion_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue. We will turn verbose back on, and then move onto defining our validation stratagy (which is again optional, but as stated before for this example we are going to preserve like family ids within the same folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ML.define_validation_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for group preserving behavior we are interested in supplying an argument for groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.verbose = True\n",
    "ML.define_validation_strategy(groups='rel_family_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when it says over 9985 unique values, this is just over all of the loaded values within ML.strat. In practice, splits will take place over only the overlap of subjects minus loaded exclusions, the above is just saying there are 9985 unique values in just strat - not the overlap.\n",
    "\n",
    "Lastly before we get to modelling, we want to define a global train-test split, so that we can perform model exploration, and parameter tuning ect... on a training set, and leave a left-out testing set to eventually test with out final selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ML.train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.train_test_split(test_size=.25, #Let be somewhat conservative, and use a size of .25\n",
    "                    random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
